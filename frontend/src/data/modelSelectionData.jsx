export default [
    {
        "name": "Convolutional Neural Network (CNN)",
        "image": "https://buffml.com/wp-content/uploads/2021/04/cnn.png",
        "description": "Convolutional Neural Networks are specialized neural networks for processing structured grid-like data such as images. They use a mathematical operation called convolution, which allows the network to detect and learn patterns in input data, making CNNs highly effective in image and video recognition tasks. "
    },
    {
        "name": "Recurrent Neural Network (RNN)",
        "image": "https://www.softwebsolutions.com/wp-content/uploads/2023/11/Blog__CNN-vs-RNN-vs-ANN.jpg",
        "description": "Recurrent Neural Networks are a class of neural networks designed for sequential data analysis. Unlike traditional feedforward neural networks"
    },
    {
        "name": "K-Nearest Neighbors (KNN)",
        "image": "https://images.datacamp.com/image/upload/v1686762721/image2_a2876c62d1.png",
        "description": "K-Nearest Neighbors is a simple, instance-based learning algorithm used for both classification and regression. KNN assumes that similar data points are close to each other and classifies new points based on the majority class of its nearest neighbors. "
    },
    {
        "name": "Random Forest",
        "image": "https://www.frontiersin.org/files/Articles/284242/fnagi-09-00329-HTML/image_m/fnagi-09-00329-g001.jpg",
        "description": "Random Forest is an ensemble learning method that constructs multiple decision trees during training. It outputs the mode of the classes (classification) or mean prediction (regression) of individual trees. Random Forest is known for its ability to handle large datasets with higher dimensionality, reducing overfitting by averaging multiple decision tree results and providing a robust prediction mechanism."
    },
    {
        "name": "Gradient Boosting Machine (GBM)",
        "image": "https://example.com/images/gbm.jpg",
        "description": "Gradient Boosting Machines are a powerful ensemble learning technique used for both classification and regression. It works by combining the outputs of weak learners (usually decision trees) to produce a strong model. GBMs iteratively optimize a loss function by adding new models that correct the errors of previous ones, which makes them highly effective but also prone to overfitting without proper tuning."
    },
    {
        "name": "Long Short-Term Memory (LSTM)",
        "image": "https://example.com/images/lstm.jpg",
        "description": "Long Short-Term Memory networks are a special type of Recurrent Neural Network (RNN) designed to overcome the limitations of traditional RNNs, particularly in learning long-term dependencies. LSTMs use gating mechanisms to regulate the flow of information, making them highly effective in tasks involving sequential data such as natural language processing, speech recognition, and time-series prediction."
    },
    {
        "name": "Recurrent Neural Network (RNN)",
        "image": "https://www.softwebsolutions.com/wp-content/uploads/2023/11/Blog__CNN-vs-RNN-vs-ANN.jpg",
        "description": "Recurrent Neural Networks are a class of neural networks designed for sequential data analysis. Unlike traditional feedforward neural networks, RNNs have connections between nodes that form a directed cycle, allowing them to maintain memory of previous inputs in a sequence. This makes RNNs well-suited for tasks such as language modeling, speech recognition, and time-series forecasting."
    },
    {
        "name": "K-Nearest Neighbors (KNN)",
        "image": "https://images.datacamp.com/image/upload/v1686762721/image2_a2876c62d1.png",
        "description": "K-Nearest Neighbors is a simple, instance-based learning algorithm used for both classification and regression. KNN assumes that similar data points are close to each other and classifies new points based on the majority class of its nearest neighbors. It's highly intuitive and easy to implement but can become inefficient on large datasets, especially when the number of dimensions increases."
    },
    // {
    //     "name": "Gradient Boosting Machine (GBM)",
    //     "image": "https://example.com/images/gbm.jpg",
    //     "description": "Gradient Boosting Machines are a powerful ensemble learning technique used for both classification and regression. It works by combining the outputs of weak learners (usually decision trees) to produce a strong model. GBMs iteratively optimize a loss function by adding new models that correct the errors of previous ones, which makes them highly effective but also prone to overfitting without proper tuning."
    // },

    // {
    //     "name": "Support Vector Machine (SVM)",
    //     "image": "https://statisticseasily.com/wp-content/webp-express/webp-images/uploads/2024/02/Machine-Learning-Support-Vector-Machines.jpg.webp",
    //     "description": "Support Vector Machine is a supervised learning model used primarily for classification tasks but can also be used for regression. It constructs a hyperplane or set of hyperplanes in a high-dimensional space that separates data into classes. SVMs are highly effective in high-dimensional spaces and are known for their ability to create a margin between the categories that maximizes separation, making it ideal for complex classification tasks."
    // },
    // {
    //     "name": "Long Short-Term Memory (LSTM)",
    //     "image": "https://example.com/images/lstm.jpg",
    //     "description": "Long Short-Term Memory networks are a special type of Recurrent Neural Network (RNN) designed to overcome the limitations of traditional RNNs, particularly in learning long-term dependencies. LSTMs use gating mechanisms to regulate the flow of information, making them highly effective in tasks involving sequential data such as natural language processing, speech recognition, and time-series prediction."
    // }
]